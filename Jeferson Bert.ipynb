{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from textwrap import wrap\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "from preprocessing.nlp import preprocess\n",
    "from preprocessing.tratamento import filter_data\n",
    "\n",
    "RANDOM_SEED = 15\n",
    "PRE_TRAINED_MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
    "MAX_LEN = 156\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TCEDataset(Dataset):\n",
    "    def __init__(self, empenho, targets, tokenizer, max_len):\n",
    "        self.empenho = empenho\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.empenho)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        empenho = str(self.empenho[item])\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            empenho,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'empenho_text': empenho,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = TCEDataset(\n",
    "        empenho=df.empenho.to_numpy(),\n",
    "        targets=df.encodedNatureza.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "class NaturezaClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(NaturezaClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        output = self.drop(bert_output['pooler_output'])\n",
    "        return self.out(output)\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        predictions.extend(preds)\n",
    "        real_values.extend(targets)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    macro = f1_score(real_values, predictions, average='macro')\n",
    "    micro = f1_score(real_values, predictions, average='micro')\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    macro = f1_score(real_values, predictions, average='macro')\n",
    "    micro = f1_score(real_values, predictions, average='micro')\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses), macro, micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_data(\n",
    "    '../data/dadosTCE.csv',\n",
    "    '../data/norel.xlsx'\n",
    ")\n",
    "df = df[['empenho_historico', 'natureza_despesa_cod']]\n",
    "df.columns = ['empenho', 'natureza']\n",
    "df.empenho = df.empenho.apply(preprocess)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "df['encodedNatureza'] = lb.fit_transform(df.natureza)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "df_train, df_val = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df.natureza\n",
    ")\n",
    "# df_val, df_test = train_test_split(\n",
    "#     df_val,\n",
    "#     test_size=0.5,\n",
    "#     random_state=RANDOM_SEED,\n",
    "#     stratify=df_test.natureza\n",
    "# )\n",
    "\n",
    "train_data_loader = create_data_loader(\n",
    "    df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "# val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "# test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 247896 entries, 0 to 247895\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   empenho          247896 non-null  object\n",
      " 1   natureza         247896 non-null  object\n",
      " 2   encodedNatureza  247896 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaturezaClassifier(len(lb.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load('best_model_state.bin', map_location=torch.device(device)))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 1.318478096356207 accuracy 0.7291257268321356 macro 0.2953796928550659 micro 0.7291257268321356\n",
      "Val   loss 0.6629555967361357 accuracy 0.8383466229208407 macro 0.41864676881938906 micro 0.8383466229208407\n",
      "\n",
      "691.6759706219037\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.5707323919614548 accuracy 0.8583159969341947 macro 0.493751918681895 micro 0.8583159969341947\n",
      "Val   loss 0.5225246343076563 accuracy 0.8668800172114726 macro 0.5417660493874092 micro 0.8668800172114726\n",
      "\n",
      "692.0185558716456\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.43938317174049346 accuracy 0.8836146536273894 macro 0.5757667595076855 micro 0.8836146536273893\n",
      "Val   loss 0.47636953130914117 accuracy 0.8772337936505802 macro 0.5791176092665644 micro 0.8772337936505802\n",
      "\n",
      "685.4994743903478\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.3676797558601431 accuracy 0.8989667314020297 macro 0.6283685844504749 micro 0.8989667314020297\n",
      "Val   loss 0.46361524770841656 accuracy 0.8814559830036709 macro 0.616022082093047 micro 0.8814559830036709\n",
      "\n",
      "679.9412579218547\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.32338700981816015 accuracy 0.9087231381859884 macro 0.6749355159938547 micro 0.9087231381859884\n",
      "Val   loss 0.46340405230923637 accuracy 0.8835939706060321 macro 0.6315634502305839 micro 0.8835939706060321\n",
      "\n",
      "667.8793486555418\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.2922493819956524 accuracy 0.9151832279702871 macro 0.6980796035526631 micro 0.915183227970287\n",
      "Val   loss 0.4664234779266311 accuracy 0.8845486694724952 macro 0.6449700339304 micro 0.8845486694724952\n",
      "\n",
      "666.2603915691376\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.26854465076605916 accuracy 0.919920242959309 macro 0.7213021331774354 micro 0.919920242959309\n",
      "Val   loss 0.465933238740395 accuracy 0.8851134209146284 macro 0.6517640425581673 micro 0.8851134209146284\n",
      "\n",
      "666.8283970157305\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.2500366779450662 accuracy 0.9237986019466711 macro 0.7385494277613365 micro 0.923798601946671\n",
      "Val   loss 0.4700566524203813 accuracy 0.886390834890882 macro 0.6667676171985006 micro 0.886390834890882\n",
      "\n",
      "667.8297077695529\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.23483784514992484 accuracy 0.9264840629988417 macro 0.753590036526889 micro 0.9264840629988417\n",
      "Val   loss 0.4707075187707025 accuracy 0.8853689037098791 macro 0.6745181227094387 micro 0.8853689037098791\n",
      "\n",
      "668.3756369153658\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.22363978227092418 accuracy 0.9294057985212676 macro 0.7626171308413434 micro 0.9294057985212676\n",
      "Val   loss 0.4693027281763894 accuracy 0.8868345681668437 macro 0.6805492377090162 micro 0.8868345681668437\n",
      "\n",
      "713.9496521552404\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    starting = time.time()\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss, train_macro, train_micro = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    print(\n",
    "        f'Train loss {train_loss} accuracy {train_acc} macro {train_macro} micro {train_micro}')\n",
    "    val_acc, val_loss, val_macro, val_micro = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    print(\n",
    "        f'Val   loss {val_loss} accuracy {val_acc} macro {val_macro} micro {val_micro}')\n",
    "    print()\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_macro'].append(train_macro)\n",
    "    history['train_micro'].append(train_micro)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_macro'].append(val_macro)\n",
    "    history['val_micro'].append(val_micro)\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc\n",
    "    print(f'{(time.time()-starting)/60}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('Acc.png')\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(history['train_loss'], label='train loss')\n",
    "plt.plot(history['val_loss'], label='validation loss')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('Loss.png')\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(history['train_macro'], label='train macro')\n",
    "plt.plot(history['val_macro'], label='validation macro')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Macro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('Macro.png')\n",
    "\n",
    "plt.cla()\n",
    "plt.plot(history['train_micro'], label='train micro')\n",
    "plt.plot(history['val_micro'], label='validation micro')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Micro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.savefig('Micro.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8868345681668437\n",
      "Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.93      0.80       183\n",
      "           1       0.71      0.89      0.79        19\n",
      "           2       0.98      0.90      0.93       229\n",
      "           3       0.95      1.00      0.97        35\n",
      "           4       0.81      0.91      0.86       394\n",
      "           5       0.95      0.90      0.93        62\n",
      "           6       0.62      0.90      0.73       172\n",
      "           7       0.89      0.89      0.89        19\n",
      "           8       1.00      1.00      1.00        13\n",
      "           9       0.98      0.99      0.99       180\n",
      "          10       0.78      0.97      0.86       229\n",
      "          11       0.65      0.99      0.78       124\n",
      "          12       0.35      0.30      0.32        20\n",
      "          13       0.75      0.99      0.85       210\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.97      0.99      0.98       179\n",
      "          16       0.86      1.00      0.92        18\n",
      "          17       1.00      0.21      0.35        19\n",
      "          18       1.00      1.00      1.00        19\n",
      "          19       0.87      0.56      0.68       151\n",
      "          20       0.92      0.63      0.75        19\n",
      "          21       0.89      0.79      0.83       377\n",
      "          22       0.95      0.95      0.95        39\n",
      "          23       0.38      0.04      0.07        82\n",
      "          24       0.96      0.97      0.97       103\n",
      "          25       0.96      0.69      0.80       201\n",
      "          26       1.00      0.35      0.51       107\n",
      "          27       0.49      0.61      0.54        28\n",
      "          28       0.95      0.62      0.75       177\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.99      0.98      0.99       148\n",
      "          31       1.00      0.89      0.94        19\n",
      "          32       1.00      0.83      0.91        18\n",
      "          33       0.56      1.00      0.72        19\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.86      0.71      0.78        45\n",
      "          36       1.00      1.00      1.00         4\n",
      "          37       0.62      1.00      0.76        37\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.96      0.79      0.87        33\n",
      "          40       0.97      0.97      0.97        38\n",
      "          41       0.57      0.84      0.68        19\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       0.97      0.96      0.97       258\n",
      "          45       0.96      0.97      0.97       151\n",
      "          46       0.91      0.91      0.91        11\n",
      "          47       0.92      0.95      0.93        37\n",
      "          48       0.79      1.00      0.88        11\n",
      "          49       0.96      0.93      0.95      1243\n",
      "          50       0.88      0.91      0.90       879\n",
      "          51       0.92      0.92      0.92      1143\n",
      "          52       1.00      0.91      0.95        11\n",
      "          53       0.96      0.94      0.95       258\n",
      "          54       0.95      0.89      0.92       668\n",
      "          55       0.97      0.62      0.76        50\n",
      "          56       0.98      0.95      0.97        63\n",
      "          57       0.90      0.92      0.91      1793\n",
      "          58       0.93      0.93      0.93      1234\n",
      "          59       0.97      0.93      0.95      1308\n",
      "          60       0.88      0.90      0.89      1037\n",
      "          61       0.98      0.97      0.98      1349\n",
      "          62       0.92      0.92      0.92       957\n",
      "          63       0.97      0.94      0.95       367\n",
      "          64       0.93      0.95      0.94      1153\n",
      "          65       0.98      0.97      0.97       975\n",
      "          66       0.97      0.97      0.97      1382\n",
      "          67       0.99      0.99      0.99       807\n",
      "          68       0.98      0.75      0.85        79\n",
      "          69       0.95      0.95      0.95      1227\n",
      "          70       0.92      0.90      0.91       344\n",
      "          71       1.00      1.00      1.00        17\n",
      "          72       0.96      0.84      0.90        89\n",
      "          73       1.00      1.00      1.00         8\n",
      "          74       0.95      0.68      0.79        90\n",
      "          75       0.80      0.66      0.73       100\n",
      "          76       0.83      0.91      0.87        11\n",
      "          77       0.50      0.50      0.50         6\n",
      "          78       0.87      0.75      0.81        73\n",
      "          79       0.00      0.00      0.00         1\n",
      "          80       0.80      0.98      0.88       325\n",
      "          81       0.80      0.83      0.81       142\n",
      "          82       0.85      0.76      0.80       205\n",
      "          83       0.92      0.88      0.90       135\n",
      "          84       0.96      0.93      0.94        91\n",
      "          85       0.94      0.87      0.91       110\n",
      "          86       1.00      0.74      0.85        43\n",
      "          87       1.00      1.00      1.00        30\n",
      "          88       0.98      0.98      0.98       841\n",
      "          89       0.94      0.92      0.93      1253\n",
      "          90       0.95      0.92      0.94       478\n",
      "          91       0.95      0.96      0.95      1410\n",
      "          92       0.91      0.92      0.92       401\n",
      "          93       0.94      1.00      0.97        15\n",
      "          94       0.90      0.92      0.91        84\n",
      "          95       0.83      0.91      0.87        11\n",
      "          96       0.96      0.86      0.91        51\n",
      "          97       0.80      0.83      0.81       397\n",
      "          98       1.00      0.96      0.98        28\n",
      "          99       0.85      0.85      0.85        27\n",
      "         100       0.91      0.94      0.93       669\n",
      "         101       0.75      0.60      0.67        10\n",
      "         102       1.00      0.97      0.98        94\n",
      "         103       0.91      0.91      0.91        34\n",
      "         104       0.84      0.91      0.87        56\n",
      "         105       0.84      0.92      0.88        50\n",
      "         106       1.00      0.91      0.95        43\n",
      "         107       0.83      1.00      0.91         5\n",
      "         108       0.94      0.78      0.85       152\n",
      "         109       0.00      0.00      0.00         1\n",
      "         110       0.93      0.80      0.86        93\n",
      "         111       0.20      0.76      0.32        38\n",
      "         112       0.64      0.67      0.65       102\n",
      "         113       0.76      0.74      0.75        92\n",
      "         114       0.95      0.95      0.95        56\n",
      "         115       0.81      0.96      0.88        27\n",
      "         116       0.77      0.48      0.59        21\n",
      "         117       0.85      0.85      0.85       300\n",
      "         118       0.95      0.96      0.96       181\n",
      "         119       0.91      0.97      0.94        31\n",
      "         120       0.95      0.93      0.94        40\n",
      "         121       0.90      0.87      0.89      1048\n",
      "         122       0.78      0.83      0.80       192\n",
      "         123       1.00      1.00      1.00         8\n",
      "         124       0.81      0.91      0.86        53\n",
      "         125       0.33      0.67      0.44         3\n",
      "         126       0.81      0.81      0.81        42\n",
      "         127       1.00      0.33      0.50         3\n",
      "         128       0.00      0.00      0.00         2\n",
      "         129       0.80      0.87      0.84       159\n",
      "         130       0.91      0.94      0.93       158\n",
      "         131       0.88      0.88      0.88         8\n",
      "         132       0.67      0.29      0.40         7\n",
      "         133       0.80      0.80      0.80        10\n",
      "         134       1.00      0.33      0.50         3\n",
      "         135       1.00      1.00      1.00         8\n",
      "         136       0.92      1.00      0.96        46\n",
      "         137       0.50      0.71      0.59         7\n",
      "         138       0.00      0.00      0.00         1\n",
      "         139       0.61      0.54      0.57       138\n",
      "         140       0.00      0.00      0.00         4\n",
      "         141       0.87      0.90      0.88       473\n",
      "         142       0.92      1.00      0.96        48\n",
      "         143       0.98      0.98      0.98       165\n",
      "         144       0.57      0.80      0.67         5\n",
      "         145       0.95      0.95      0.95        78\n",
      "         146       0.67      0.25      0.36         8\n",
      "         147       1.00      1.00      1.00         2\n",
      "         148       0.95      0.97      0.96      1154\n",
      "         149       0.93      0.90      0.91       660\n",
      "         150       0.97      0.93      0.95       133\n",
      "         151       0.90      0.90      0.90        67\n",
      "         152       0.89      0.91      0.90        46\n",
      "         153       1.00      0.88      0.93         8\n",
      "         154       0.67      0.36      0.47        11\n",
      "         155       0.84      0.93      0.88        28\n",
      "         156       1.00      0.67      0.80        15\n",
      "         157       1.00      0.67      0.80         3\n",
      "         158       0.99      1.00      1.00      1793\n",
      "         159       1.00      1.00      1.00         3\n",
      "         160       1.00      1.00      1.00        25\n",
      "         161       0.67      0.29      0.40         7\n",
      "         162       0.97      0.95      0.96       773\n",
      "         163       0.93      1.00      0.97        28\n",
      "         164       0.97      0.95      0.96        77\n",
      "         165       0.92      0.87      0.90       236\n",
      "         166       0.92      0.88      0.90       223\n",
      "         167       0.87      0.88      0.87       281\n",
      "         168       1.00      0.82      0.90        11\n",
      "         169       0.82      0.83      0.82      1294\n",
      "         170       0.64      0.71      0.67        91\n",
      "         171       0.00      0.00      0.00         5\n",
      "         172       0.60      0.63      0.62       115\n",
      "         173       0.59      0.64      0.62       763\n",
      "         174       0.45      0.57      0.50       466\n",
      "         175       0.59      0.48      0.53       290\n",
      "         176       0.65      0.53      0.58       179\n",
      "         177       0.69      0.56      0.62        79\n",
      "         178       0.49      0.67      0.57        45\n",
      "         179       0.86      0.84      0.85        70\n",
      "         180       0.83      0.75      0.79       151\n",
      "         181       0.84      0.85      0.85       257\n",
      "         182       0.94      0.88      0.91        52\n",
      "         183       0.73      0.44      0.55        43\n",
      "         184       0.60      0.30      0.40        10\n",
      "         185       0.84      0.73      0.78       111\n",
      "         186       0.55      0.70      0.61      1078\n",
      "         187       0.77      0.66      0.71       439\n",
      "         188       0.78      0.70      0.74       266\n",
      "         189       0.71      0.80      0.75        25\n",
      "         190       0.51      0.39      0.45       109\n",
      "         191       0.80      0.57      0.67         7\n",
      "         192       0.73      0.75      0.74        40\n",
      "         193       0.77      0.88      0.82        26\n",
      "         194       1.00      0.29      0.44         7\n",
      "         195       0.40      0.46      0.43       279\n",
      "         196       0.68      0.81      0.74        21\n",
      "         197       0.33      0.38      0.36        55\n",
      "         198       1.00      0.33      0.50         3\n",
      "         199       0.89      0.85      0.87        20\n",
      "         200       0.83      0.80      0.81       708\n",
      "         201       0.85      0.62      0.72        84\n",
      "         202       0.77      0.65      0.70        31\n",
      "         203       0.66      0.48      0.56        99\n",
      "         204       0.71      0.26      0.38        19\n",
      "         205       0.56      0.42      0.48        36\n",
      "         206       0.64      0.70      0.67        33\n",
      "         207       0.00      0.00      0.00         1\n",
      "         208       0.00      0.00      0.00         9\n",
      "         209       0.86      0.82      0.84        60\n",
      "         210       0.00      0.00      0.00        18\n",
      "         211       0.91      0.86      0.88        83\n",
      "         212       1.00      1.00      1.00         4\n",
      "         213       1.00      0.50      0.67         4\n",
      "         214       0.65      0.66      0.65        47\n",
      "         215       0.67      0.80      0.73         5\n",
      "         216       1.00      0.40      0.57         5\n",
      "         217       1.00      1.00      1.00        12\n",
      "         218       0.85      0.86      0.85       464\n",
      "         219       0.00      0.00      0.00         1\n",
      "         220       0.00      0.00      0.00         2\n",
      "         221       0.64      0.71      0.67        41\n",
      "         222       0.56      0.50      0.53        10\n",
      "         223       1.00      0.50      0.67         4\n",
      "         224       1.00      0.62      0.77         8\n",
      "         225       0.86      0.99      0.92       218\n",
      "         226       0.96      0.84      0.90        31\n",
      "         227       0.97      0.96      0.96       445\n",
      "         228       0.71      0.71      0.71        14\n",
      "         229       0.89      0.87      0.88        46\n",
      "         230       1.00      1.00      1.00        20\n",
      "         231       0.95      0.93      0.94        87\n",
      "         232       0.65      0.77      0.71        22\n",
      "         233       0.50      0.82      0.62        11\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       1.00      0.40      0.57         5\n",
      "         236       0.00      0.00      0.00         1\n",
      "         237       0.00      0.00      0.00         3\n",
      "         238       0.67      0.57      0.62         7\n",
      "         239       0.00      0.00      0.00         2\n",
      "         240       0.00      0.00      0.00         1\n",
      "         241       1.00      0.33      0.50         3\n",
      "         242       0.00      0.00      0.00         1\n",
      "         243       0.00      0.00      0.00         2\n",
      "         244       0.96      0.83      0.89        30\n",
      "         245       0.95      0.98      0.97       929\n",
      "         246       0.00      0.00      0.00        14\n",
      "         247       0.00      0.00      0.00         5\n",
      "         248       0.00      0.00      0.00         1\n",
      "         249       1.00      1.00      1.00       271\n",
      "         250       0.00      0.00      0.00         3\n",
      "         251       1.00      0.75      0.86         4\n",
      "         252       0.00      0.00      0.00         1\n",
      "         253       1.00      0.94      0.97        63\n",
      "         254       0.96      0.98      0.97        83\n",
      "         255       0.97      0.95      0.96        40\n",
      "         256       0.97      0.98      0.98        66\n",
      "         257       0.00      0.00      0.00         2\n",
      "         258       0.94      0.78      0.85        40\n",
      "         259       0.89      0.89      0.89       131\n",
      "         260       0.00      0.00      0.00         1\n",
      "         261       1.00      0.33      0.50         3\n",
      "         262       1.00      0.40      0.57         5\n",
      "         263       0.90      0.82      0.86        11\n",
      "         264       0.00      0.00      0.00         2\n",
      "         265       0.96      1.00      0.98       114\n",
      "         266       0.67      0.86      0.75         7\n",
      "         267       0.57      0.37      0.45        46\n",
      "         268       0.54      0.78      0.64         9\n",
      "         269       0.96      0.98      0.97       263\n",
      "         270       0.88      0.78      0.82        18\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       1.00      0.60      0.75         5\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         1\n",
      "         275       0.92      0.94      0.93       289\n",
      "         276       0.97      0.90      0.93       236\n",
      "         277       0.72      0.74      0.73        97\n",
      "         278       0.88      0.87      0.88       167\n",
      "         279       1.00      0.96      0.98        23\n",
      "         280       0.00      0.00      0.00         5\n",
      "         281       0.85      0.87      0.86        38\n",
      "         282       0.87      0.84      0.85        31\n",
      "         283       0.00      0.00      0.00         5\n",
      "         284       0.86      0.89      0.88       126\n",
      "         285       0.99      0.99      0.99       366\n",
      "         286       0.92      0.94      0.93       233\n",
      "         287       0.99      0.99      0.99       949\n",
      "         288       0.87      0.90      0.88       222\n",
      "         289       0.67      0.85      0.75        48\n",
      "         290       0.83      0.74      0.78        34\n",
      "         291       0.87      0.94      0.91       139\n",
      "         292       0.91      0.79      0.85       140\n",
      "         293       0.96      0.93      0.94       425\n",
      "         294       0.87      0.92      0.89       122\n",
      "         295       0.66      0.64      0.65       381\n",
      "         296       0.74      0.76      0.75       690\n",
      "         297       0.99      0.98      0.98       210\n",
      "         298       0.58      0.64      0.61       319\n",
      "         299       0.84      0.81      0.83       326\n",
      "         300       1.00      0.96      0.98        28\n",
      "         301       0.99      0.99      0.99      1298\n",
      "         302       0.97      0.97      0.97        77\n",
      "         303       0.86      0.70      0.77        60\n",
      "         304       1.00      0.71      0.83         7\n",
      "         305       0.00      0.00      0.00         1\n",
      "         306       0.87      0.87      0.87        38\n",
      "         307       0.96      0.99      0.98       377\n",
      "         308       0.94      0.92      0.93       158\n",
      "         309       0.84      0.72      0.78       269\n",
      "         310       0.85      0.85      0.85       239\n",
      "         311       0.98      0.95      0.96       131\n",
      "         312       0.73      0.75      0.74        80\n",
      "         313       0.94      0.94      0.94       164\n",
      "         314       0.92      0.95      0.94       325\n",
      "         315       0.86      0.86      0.86        14\n",
      "         316       0.98      0.99      0.98       124\n",
      "         317       0.91      0.81      0.86        37\n",
      "         318       0.50      0.50      0.50        10\n",
      "         319       1.00      1.00      1.00         7\n",
      "         320       0.95      0.79      0.86        52\n",
      "         321       1.00      0.96      0.98        92\n",
      "         322       0.00      0.00      0.00         1\n",
      "         323       0.00      0.00      0.00         1\n",
      "         324       0.94      0.94      0.94        16\n",
      "         325       0.93      0.82      0.87       191\n",
      "         326       0.00      0.00      0.00         4\n",
      "         327       0.94      0.85      0.89        34\n",
      "         328       0.00      0.00      0.00         3\n",
      "         329       0.97      0.97      0.97       182\n",
      "         330       0.93      0.87      0.90        15\n",
      "         331       0.87      0.87      0.87       163\n",
      "         332       0.70      0.61      0.65        46\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.88      0.88      0.88        32\n",
      "         335       0.87      0.72      0.79        75\n",
      "         336       0.00      0.00      0.00        15\n",
      "         337       1.00      1.00      1.00        10\n",
      "         338       0.87      0.91      0.89        22\n",
      "         339       0.94      0.96      0.95        50\n",
      "         340       0.57      0.76      0.65        42\n",
      "         341       0.56      0.62      0.59         8\n",
      "         342       0.99      0.99      0.99       267\n",
      "         343       0.98      0.95      0.96        56\n",
      "         344       0.56      0.62      0.59         8\n",
      "         345       0.94      0.93      0.94       366\n",
      "         346       0.89      0.91      0.90        65\n",
      "         347       0.86      0.86      0.86         7\n",
      "         348       0.86      0.94      0.90       146\n",
      "         349       0.00      0.00      0.00         3\n",
      "         350       0.00      0.00      0.00         5\n",
      "         351       0.64      0.60      0.62        15\n",
      "         352       0.70      0.58      0.63        33\n",
      "         353       0.68      0.79      0.73        53\n",
      "         354       0.91      0.89      0.90       104\n",
      "         355       0.61      0.70      0.65        43\n",
      "         356       0.97      0.98      0.97       166\n",
      "         357       0.97      0.96      0.97       356\n",
      "         358       0.96      0.95      0.95       682\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.68      0.66      0.67       116\n",
      "         361       0.88      0.93      0.90       647\n",
      "         362       0.97      0.78      0.87        37\n",
      "         363       1.00      1.00      1.00       203\n",
      "         364       0.97      0.99      0.98       299\n",
      "         365       0.93      0.96      0.95       204\n",
      "         366       0.92      0.87      0.89       414\n",
      "         367       0.00      0.00      0.00         1\n",
      "         368       0.99      0.99      0.99       199\n",
      "         369       1.00      0.96      0.98        26\n",
      "         370       0.82      1.00      0.90        28\n",
      "         371       1.00      1.00      1.00         3\n",
      "         372       0.88      0.95      0.91       326\n",
      "         373       0.92      0.61      0.73        18\n",
      "         374       0.87      0.92      0.89        71\n",
      "         375       0.96      0.86      0.91        29\n",
      "         376       0.86      0.86      0.86        22\n",
      "         377       1.00      0.95      0.98        22\n",
      "         378       0.95      0.91      0.93        22\n",
      "         379       1.00      1.00      1.00        22\n",
      "         380       0.99      0.96      0.97       462\n",
      "         381       0.94      0.95      0.95       371\n",
      "         382       0.95      0.98      0.96       453\n",
      "         383       0.89      0.83      0.86        59\n",
      "         384       0.88      0.87      0.87       644\n",
      "         385       0.93      0.94      0.94       143\n",
      "         386       0.93      0.89      0.91       330\n",
      "         387       0.96      0.96      0.96      1367\n",
      "         388       0.88      0.92      0.90       261\n",
      "         389       0.70      0.83      0.76        46\n",
      "         390       0.00      0.00      0.00         2\n",
      "         391       1.00      1.00      1.00        14\n",
      "         392       0.00      0.00      0.00         1\n",
      "         393       0.95      0.95      0.95        22\n",
      "         394       0.96      1.00      0.98        55\n",
      "         395       0.82      1.00      0.90         9\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       1.00      1.00      1.00         2\n",
      "         398       1.00      1.00      1.00         2\n",
      "         399       1.00      0.62      0.77         8\n",
      "         400       0.67      0.67      0.67         6\n",
      "         401       0.00      0.00      0.00         1\n",
      "         402       0.80      1.00      0.89         4\n",
      "         403       0.86      0.86      0.86        28\n",
      "         404       0.75      0.60      0.67         5\n",
      "         405       0.68      0.91      0.78        54\n",
      "         406       0.50      0.33      0.40         3\n",
      "         407       0.00      0.00      0.00         1\n",
      "         408       0.96      0.98      0.97      1066\n",
      "         409       0.87      0.65      0.74        31\n",
      "         410       0.00      0.00      0.00         1\n",
      "         411       0.00      0.00      0.00         1\n",
      "         412       0.33      0.06      0.10        18\n",
      "         413       0.00      0.00      0.00         1\n",
      "         414       0.83      0.81      0.82       142\n",
      "         415       0.86      0.71      0.78        79\n",
      "         416       0.93      0.97      0.95       294\n",
      "         417       0.44      0.29      0.35        14\n",
      "         418       1.00      0.60      0.75         5\n",
      "         419       0.35      0.46      0.40        28\n",
      "         420       0.00      0.00      0.00         3\n",
      "         421       0.50      0.42      0.45        12\n",
      "         422       0.00      0.00      0.00         4\n",
      "         423       0.00      0.00      0.00         5\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       0.50      0.38      0.43        13\n",
      "         426       0.00      0.00      0.00         2\n",
      "         427       0.67      0.50      0.57         4\n",
      "         428       0.88      0.64      0.74        11\n",
      "         429       0.92      0.93      0.92        72\n",
      "         430       0.84      0.76      0.80        21\n",
      "         431       1.00      0.98      0.99        42\n",
      "         432       1.00      0.92      0.96        24\n",
      "         433       0.97      0.93      0.95        67\n",
      "         434       0.98      0.96      0.97       189\n",
      "         435       1.00      0.86      0.92         7\n",
      "         436       0.78      0.70      0.74        10\n",
      "         437       0.00      0.00      0.00         2\n",
      "         438       1.00      0.50      0.67         2\n",
      "         439       0.75      0.86      0.80         7\n",
      "         440       0.00      0.00      0.00         1\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.99      0.98      0.98       228\n",
      "         443       0.64      0.88      0.74        16\n",
      "         444       0.92      1.00      0.96        22\n",
      "         445       0.52      0.92      0.67        13\n",
      "         446       0.00      0.00      0.00         1\n",
      "         447       0.79      0.76      0.78        84\n",
      "         448       0.78      0.78      0.78        65\n",
      "         449       0.34      0.56      0.43        36\n",
      "         450       0.33      0.25      0.29         4\n",
      "         451       0.67      0.33      0.44         6\n",
      "         452       0.89      0.89      0.89        38\n",
      "         453       1.00      0.11      0.20         9\n",
      "         454       0.82      1.00      0.90         9\n",
      "         455       0.91      0.86      0.88        58\n",
      "         456       0.82      0.78      0.80       241\n",
      "         457       0.51      0.64      0.56        58\n",
      "         458       0.59      0.53      0.56        60\n",
      "         459       0.73      0.81      0.77       104\n",
      "         460       0.75      0.86      0.80         7\n",
      "         461       0.89      0.86      0.87        28\n",
      "         462       0.00      0.00      0.00         7\n",
      "         463       1.00      0.14      0.25         7\n",
      "         464       0.00      0.00      0.00         1\n",
      "         465       0.20      0.15      0.17        13\n",
      "         466       0.45      0.53      0.49        17\n",
      "         467       0.75      0.79      0.77       204\n",
      "         468       0.00      0.00      0.00         5\n",
      "         469       0.83      0.85      0.84       205\n",
      "         470       0.00      0.00      0.00         3\n",
      "         471       0.65      0.60      0.62        47\n",
      "         472       0.78      0.76      0.77        76\n",
      "         473       1.00      0.33      0.50         3\n",
      "         474       0.94      1.00      0.97        33\n",
      "         475       0.00      0.00      0.00         2\n",
      "         476       0.00      0.00      0.00         5\n",
      "         477       0.00      0.00      0.00         4\n",
      "         478       0.88      0.78      0.82         9\n",
      "         479       0.00      0.00      0.00         1\n",
      "         480       0.00      0.00      0.00         1\n",
      "         481       0.00      0.00      0.00         2\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       1.00      1.00      1.00         7\n",
      "         484       0.00      0.00      0.00         2\n",
      "         485       0.69      0.66      0.68        38\n",
      "         486       0.67      1.00      0.80         2\n",
      "         487       1.00      0.87      0.93        15\n",
      "         488       1.00      1.00      1.00         1\n",
      "         489       0.00      0.00      0.00         1\n",
      "         490       1.00      1.00      1.00        65\n",
      "         491       0.00      0.00      0.00         1\n",
      "         492       1.00      0.88      0.93         8\n",
      "         493       0.94      0.86      0.90        59\n",
      "         494       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.89     74369\n",
      "   macro avg       0.71      0.67      0.68     74369\n",
      "weighted avg       0.89      0.89      0.89     74369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_acc, _, _, _ = eval_model(\n",
    "    model,\n",
    "#     test_data_loader,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "print(test_acc.item())\n",
    "\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"empenho_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values\n",
    "\n",
    "\n",
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    val_data_loader\n",
    ")\n",
    "\n",
    "print('Test Classification Report')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {},
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}